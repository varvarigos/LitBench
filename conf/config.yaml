tokenizer:
  max_length: 1024

qlora:
  rank: 8
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:  # modules for which to train lora adapters
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  #- gate_proj
  #- up_proj
  #- down_proj

directories:
  pretrained_model: "models/llama_1b_qlora_uncensored_1_adapter_test_graph"
  save_zip_directory: "quant_bio_retrieval/research_papers_zip/"
  save_directory: "quant_bio_retrieval/research_papers/"
  save_description: "quant_bio_retrieval/description/"
  save_graph: "quant_bio_retrieval/description/test_graph.gexf"
  gexf_file: "quant_bio_retrieval/description/test_graph.gexf"
  metadata_path: "arxiv-metadata-oai-snapshot.json"
  retrieval_nodes_path: "datasets/retrieval_nodes.json"
  predefined_graph_path: "datasets/bio_noai.gexf"

processing:
  random_seed: 10
  keep_unstructured_content: true

model_name: llama_1b_qlora_uncensored
base_model: meta-llama/Llama-3.2-1B
model_family: llama  # if unspecified will use AutoModelForCausalLM/AutoTokenizer
trainer_output_dir: trainer_outputs/
eval_output_dir: eval_outputs/
model_output_dir: models   # model saved in {model_output_dir}/{model_name}
