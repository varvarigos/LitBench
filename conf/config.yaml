tokenizer:
  max_length: 1024

qlora:
  rank: 8
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:  # modules for which to train lora adapters
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  #- gate_proj
  #- up_proj
  #- down_proj

training:
  per_device_train_batch_size: 4
  warmup_steps: 100
  num_train_epochs: 1
  learning_rate: 0.0002
  lr_scheduler_type: 'cosine'
  fp16: true
  logging_steps: 1
  report_to: "wandb"
  save_steps: 50

generation:
  max_new_tokens: 1000
  do_sample: True
  top_p: 0.9
  top_k: 50
  temperature: 0.7
  no_repeat_ngram_size: 2
  num_beams: 1

directories:
  pretrained_model: "models/llama_1b_qlora_uncensored_1_adapter_test_graph"
  save_zip_directory: "quant_bio_retrieval/research_papers_zip/"
  save_directory: "quant_bio_retrieval/research_papers/"
  save_description: "quant_bio_retrieval/description/"
  save_graph: "quant_bio_retrieval/description/test_graph.gexf"
  gexf_file: "quant_bio_retrieval/description/test_graph.gexf"
  metadata_path: "arxiv-metadata-oai-snapshot.json"
  retrieval_nodes_path: "datasets/retrieval_nodes.json"
  predefined_graph_path: "datasets/bio_noai.gexf"

processing:
  random_seed: 10
  keep_unstructured_content: true

model_name: llama_1b_qlora_uncensored
base_model: meta-llama/Llama-3.2-1B
model_family: llama  # if unspecified will use AutoModelForCausalLM/AutoTokenizer
trainer_output_dir: trainer_outputs/
eval_output_dir: eval_outputs/
model_output_dir: models   # model saved in {model_output_dir}/{model_name}
